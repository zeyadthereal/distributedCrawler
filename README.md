# ğŸŒ Distributed Web Crawling and Indexing System

This project is a **cloud-native distributed web crawler and indexing system**, designed to efficiently crawl websites starting from seed URLs, extract and store raw HTML content hierarchically, and build a searchable index using Elasticsearch.

The architecture is modular, scalable, and fault-tolerant â€” built using **Python**, **Flask**, **Docker**, and **Kubernetes**, and integrated with **Elasticsearch** and **Kibana** for indexing and visualization.

---

## ğŸš€ Project Overview

This system allows users to:
- Submit one or more seed URLs to start a crawl
- Automatically distribute crawling tasks across multiple worker pods
- Extract and store HTML content hierarchically under each seed
- Index content into Elasticsearch for full-text search
- Visualize indexed data using Kibana
- Deploy and scale effortlessly via Kubernetes

All application components are already **containerized** and available on **Docker Hub** â€” only the Kubernetes manifests are needed for deployment.

---

## ğŸ§± System Components

### ğŸ”¹ Client Node (`Client-Node`)
- Flask-based web interface
- Provides endpoints to submit crawl requests and monitor system status

### ğŸ”¹ Master Node (`Master-Node`)
- Coordinates task distribution to crawlers
- Monitors crawler status and reassigns failed tasks
- Sends crawled content to the indexer

### ğŸ”¹ Crawler Nodes (`Crawler-Node`)
- Python-based workers
- Fetch URLs, extract links and raw HTML
- Communicate results to the master and indexer

### ğŸ”¹ Indexing & Search
- **Elasticsearch**: Stores structured crawl data
- **Kibana**: Visualizes and searches indexed content

### ğŸ”¹ Kubernetes Specifications (`k8s-specifications`)
- All Kubernetes manifests for deploying the system (Deployments, Services, etc.)

---

## ğŸ§° Technology Stack

| Layer                | Technology            |
|----------------------|------------------------|
| Crawling and parsing | Python                 |
| Web Interface        | Flask                  |
| Indexing Engine      | Elasticsearch          |
| Visualization        | Kibana                 |
| Containerization     | Docker                 |
| Orchestration        | Kubernetes (GKE) |

---

## ğŸ“¦ Project Structure

```bash
.
â”œâ”€â”€ Client-Node/            # Flask app (client interface)
â”œâ”€â”€ Master-Node/            # Master node code (scheduler/controller)
â”œâ”€â”€ Crawler-Node/           # Distributed web crawlers
â””â”€â”€ k8s-specifications/     # All Kubernetes deployment YAMLs
```

## ğŸ“„ Deployment Guide

  ### ğŸ”¹ Create A Kubernetes Cluster in GCP
  - Go to Kubernetes Engine
  - Click on Create
  - Apply the desired configurations for your K8s cluster
  - Wait for the nodes to spawn
  
  ### ğŸ”¹ Access Kubectl to Control the Node Pool
  - Spawn your google cloud shell
  - Enter the following command
  ```bash
  gcloud container clusters get-credentials <YOUR_CLUSTER_NAME> --zone <YOUR_ZONE> --project <YOUR_PROJECT_NAME>
  ```
  ### ğŸ”¹ Clone The Repository
    
  ```bash
    git clone https://github.com/zeyadthereal/distributedCrawler.git
    cd distributed-crawler/k8s-specifications
  ```
  ### ğŸ”¹ Deploy The Pods
  ```bash
    kubectl create -f MANIFEST_FILE_NAME.yaml
  ```
  ### ğŸ”¹ Visualize The Deployments
  ```bash
    kubectl get pods -o wide
  ```
  
  ## ğŸ‘¨â€ğŸ’» Client Interface
  ### ğŸ”¹ Access The Interface
  - You have to know the IP address on which your client is hosted
  ```bash
      kubectl get pods,svc
  ```
  - Type The IP address followed by port 5000 in the browser's search bar
  
  
  ## ğŸ” Indexer And Database
  ### ğŸ”¹ To Access The Database
  - You have to know the IP address on which elasticsearch is hosted
  - Type The IP address followed by port 9200 in the browser's search bar
  ```bash
    kubectl get pods,svc
  ```
      
  ### ğŸ”¹ To Access the GUI of The Database
  - You have to know the IP address on which elasticsearch is hosted
  - Type The IP address followed by port 5601 in the browser's search bar
 
